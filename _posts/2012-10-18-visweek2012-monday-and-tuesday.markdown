---
layout: post
title: "VisWeek 2012, Monday and Tuesday"
---
This continues the set of notes I started on the
previous post.

## Monday

It turns out I spent most of Monday hanging out on hallways and
chatting with people, so I did not really see much of the
sessions. But I will highlight one bit about the BioVis sessions which
I thought was pretty great (Robert
[mentioned](http://eagereyes.org/blog/2012/visweek-2012-day-one)
this as well on his post). [BioVis](http://biovis.net/) was run as a traditional symposium,
with a set of closely-related papers on a specific area (in this case,
biological data visualization). The brilliant idea by the organizers
was to start each session with a short presentation by the session
chairs. This presentation briefly mentioned all the papers in the
session, and put them in context of the wider area. The session chair,
having had access to the work and typically more experience in the
area than the presenter, can give the wider context. This way the
audience can connect dots between the individual papers much more
easily. VisWeek should seriously consider doing something like this
instead of fast forwards. As much as I enjoyed Gordon's "Tiny
Particles" uke masterpiece, or (shame on me, I forget their name) "The devil came down to Baltimore", I think I
would be more likely to get something out of a short presentation by
the session chairs before each set of papers. 

## Tuesday

I started the day with the Infovis session on evaluation, and with
[Steve Haroz](http://steveharoz.com)'s presentation of this year's
best
Infovis paper. Haroz and Whitney designed a set of simple experiments
that show very clearly how information displays are fundamentally
bounded by our limited capacity for attention, and discuss how to optimize
visualizations to avoid squandering this limited resource. 
The [paper](http://steveharoz.com/research/attention/papers/Haroz_Whitney_2012_InfoVis.pdf) includes
all the relevant details, and you should read it. But what really
caught my attention was Haroz's presentation. I don't know if VGTC is recording
these, but if you have the opportunity to see one of his talks, I urge
you to do so: he was clear, persuasive, only explained as much as
needed in a talk (because, really, you should read the paper!), and,
above all, he used the projector to *show, not tell*. Really
great job. There has been some discussion on how to interpret the
results, and Haroz [has written a response](http://steveharoz.com/blog/?p=203). How crazy is it
that these written discussions can now happen during the course of a
conference? We truly live in the future. Also, and I'll try to put it
mildly, dropbox user 14753707: don't act like a spineless coward, and let us
know who you are! This is not how scholarly discourse happens.

The other work I want to highlight of the morning session is Hofmann, Follett, Majumder and
Cook's
[paper](http://ieeevis.org/year/2012/paper/infovis/graphical-tests-power-comparison-competing-designs) on evaluating visualization designs by appealing to the notion of
[statistical
power](http://en.wikipedia.org/wiki/Statistical_power). This is a followup and a direct application of the techniques
in
[Graphical
Inference for Infovis](https://vita.had.co.nz/papers/inference-infovis.html), which if you're at all a reader of this blog,
you're probably sick of hearing. But bear with me some more, because
this is great stuff. The original paper presented the groundbreaking
techniques for turning visual tasks into formal statistical
tests. This paper shows how this idea can be used to *compare
different visualization designs*. The basic idea is this: a
visualization design is good if it is hard to hide the true data among
"impostors" which are created by sampling bogus data with similar
distributions to the true dataset. This is almost unbelievably simple,
but it turns out to unlock many powerful (and widely studied!)
tools from statistical inference theory directly so then can be used in
visualization evaluation. As this is an area that in my opinion is in
sore lacking of techniques which generalize effectively, this
development by Hofmann and co-authors is hugely exciting.

Other cool papers I saw today included (but were not
limited to!) the work from
[Ahmed,
Zheng and Mueller](http://www.cs.sunysb.edu/~mueller/papers/HPU_vis_2012.pdf) in leveraging human computation to design better
compositing operators,
[Wu, Yuan
and Ma](http://research.microsoft.com/en-us/um/people/ycwu/)'s paper on non-independence and interaction modeling for
uncertainty displays, and
[Sedlmair, Meyer
and Munzner](http://www.cs.ubc.ca/nest/imager/tr/2012/dsm/)'s work on collecting best practices for design studies,
which are a powerful, popular, and monstrously hard way to do
applied visualization work.

Finally, the day ended with a phenomenal party, organized by
[Noah Illinsky](http://complexdiagrams.com) in connection with
the
[Seattle
Data Visualization Meetup](http://www.linkedin.com/groups/Next-meetups-Sept-25th-Oct-4421544%2ES%2E163816724) and graciously sponsored by
[Tableau Software](http://www.tableausoftware.com/) (if I'm missing
anyone else, please let me know!). This was, as Noah
put it, an attempt to bring together two groups of people with very
similar interests that would otherwise probably not overlap very
much. There were five or six talks ranging from things like "why you
should do *pro bono* visualization work and make the world a
better place" to the intersection of art and
visualization (by
[Francesca
Samsel](http://www.francescasamsel.com/home_html/HOME.html), who is actually organizing a
workshop on Thursday), to our own
[Robert Kosara](http://eagereyes.org) talking about story
telling in visualization. This was possibly the best party I've ever
attended at VisWeek, and I would love for something like it to become
a tradition. 
