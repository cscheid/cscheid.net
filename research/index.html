---
layout: paper
title: Carlos Scheidegger's Research Statement
---
<div class="container">

<div class="row">
<h1><span class="bold">COLLABORATIVE DATA ANALYSIS</span> IN
  THE <span class="bold">BIG DATA ERA</span></h1>
<span class="tk-garamond-display-pro" style="font-weight: 100">
  Carlos Scheidegger<br>November 2013<br>
</span>
</div>

<div class="row"><div class="col-md-7">
<p>I investigate computational, data-driven methods to understand and
  improve the world, and study threats to their validity and
  adoption. My principal approach is to design novel computer science
  tools and methods for <i>visualizing and exploring data at scale</i>. I
  publish my work in scientific and information visualization
  venues, where I strive to increase the utility of visual depictions of
  data and, more broadly, to improve how users of data-intensive methods
  interact with data and processes of the size and complexity they are
  presently faced.
</p>

<p>I see data visualization as central to the process of
  <i>understanding</i>, more than as a tool for presentation.  As data
  sources grow, Tukey's exploratory data analysis<a href="#ref-eda"></a>
  perspective only becomes more important: by turning to visualizations
  and other rich diagnostics, analysts hope to <i>formulate new
  hypotheses</i>, in addition to testing and measuring them. The methods
  I design make exploratory data analysis and visualization more
  general, effective, well-founded, and scalable, in dataset size and
  complexity. I believe in a synergy between traditional academic work
  in the form of scholarly articles, and the design and development of
  open source software. Often the <i>best</i> way to express some idea
  is through software; software can shape our understanding of the world
  as much as traditional scholarly discourse.</p>

<p>During my earliest graduate school work in geometry
   processing<a href="#ref-tpss, ref-drm, ref-hqe"></a> I became acutely aware of
   the difficulty in keeping track of the large number of computational
   experiments that occur in the daily ongoings of research. This work
   taught me two lessons. First, exploratory data-driven work is very
   often the bottleneck when trying to solve a computational
   problem. Second, data visualization is most valuable when it is an
   integral part of exploratory work.</p>

<p>Identifying and attacking the barriers to <i>data-driven
   exploratory work</i> has stuck with me and, in some shape or another,
   been a part of my work ever since. In my thesis work in
   VisTrails<a href="#ref-vistrails"></a>, I helped design and implement the basic
   system, together with algorithms for querying and reusing the provenance
   information<a href="#ref-querying"></a>. The standalone VisTrails
   program has now been downloaded over 40,000 times, and it has in addition
   been incorporated into software used in physics
   research<a href="#ref-alps"></a> in climate analysis<a href="#ref-uvcdat"></a>.
</p>

<p>This drive to create practical systems alongside my scholarly work is a
   central part of my vision, and persists to the present: my current
   work in large data exploration, trajectory clustering and
   collaborative data analysis is being used at AT&amp;T both in research
   and in business units. </p>

</div></div>

<div class="row">
<h1>INTERACTIVE DATA EXPLORATION AT SCALE</h1>
</div>

</div>

<div class="jumbotron" style="background:#fff; padding: 0; margin: 0"><iframe src="nc/view.html#twitter" height=400 width=100%></iframe></div>

<div class="container">
<div class="row"><div class="col-md-7">
<p>A crucial barrier for enabling exploratory work in today's practice is
the sheer scale of the data with which we are face.
Recently I, together with Lauro Lins and James Klosowski, introduced the
idea of <i>nanocubes</i><a href="#ref-nanocubes"></a>. Nanocubes
are an efficient data structure for multiscale, spatiotemporal data
visualization and exploration. 
They are a significant improvement on the idea of <i>data
  cubes</i>, pioneered by Jim Gray and
colleagues<a href="#ref-datacube"></a>. Nanocubes avoid the exponential blowup
(owing to the inclusion lattice of the columns) in many practical
cases by a modified insertion procedure that coalesces different
references on the data cube which point to the same subset of rows of
the original relation. We improved the results of Sismanis and
co-authors<a href="#ref-dwarf-cube"></a>, and designed nanocubes specifically
for the multi-scale nature of spatiotemporal data exploration:
sometimes an anomaly of interest spans an entire continent, and
sometimes a spike happens in a single day around a city block.</p>

<p>Nanocubes enable interactive visualizations of hundreds of millions to
billions of data points, on devices varying from workstations to
laptops, tablets, and smartphones. For visualizations of this kind and
generality, nanocubes push the current state of the art for
by around two orders of magnitude. </p>

<p>Sometimes exploratory data analysis is hampered by the lack of scale
in handling raw data. Just as often, however, it is the algorithms which
process the data to generate derived visualizations that do not
scale. Such is the case with trajectory clustering.
In collaboration with James Klosowski, Nivan Ferreira and
Cl&aacute;udio Silva, I have developed a novel algorithm for
clustering large trajectory datasets, which we call vector-field
$k$-means.  </p>

<p>Our technique uses insights from geometry processing and numerical
analysis (specifically, the Galerkin projection method), retaining
both the infinite-dimensional character of the problem (each
trajectory is itself a <i>function</i> from an interval to the plane)
and the computational simplicity of the $k$-means method. Vector-field
$k$-means<a href="#ref-vfkm"></a> is comparably fast to fastest methods in the
literature, while achieving the clustering quality of the best
methods. Our main insight is to cluster trajectories indirectly, by
means of finding a set of smooth vector fields which agree with the
tangent vectors of the trajectories. This indirection provides not
only the computational simplicity, but allows us to reconstruct
direction fields from partial trajectories.
Previous state-of-the-art techniques either have no convexity
structure and fail to have efficient
implementations<a href="#ref-icip-trajectory"></a>, or perform ad-hoc
transformations of the trajectory information into a
finite-dimensional space<a href="#ref-traclus"></a>. </p>

<p>The method has successfully been used to classify anonymous cellular
radio handoff patterns into different kinds of highway traffic. Here,
the data is as massive --- hundreds of thousands of trajectories, each
with hundreds of vertices and locations --- as it is noisy: observed
trajectories are necessarily polygonal lines along cell towers, and so
only very roughly approximate any actual trajectory. Before
vector-field $k$-means, this analysis was simply impossible:
methods that were fast enough could not cope with the noise, but
methods that could have been good enough could not cope with the
scale.</p>

</div></div>

<div class="row">
<h1>BETTER VISUALIZATIONS VIA APPROXIMATE LINEAR ALGEBRA</h1>
</div>

<div class="row"><div class="col-md-7">
<p>Even though some visualization algorithms are fast, it is just as
important that they be <i>effective</i>, and often the best visualization
algorithms are not fast enough to cope with the scale of data in
today's data analysis tasks.
In collaboration with Shankar Krishnan, Yifan Hu and Marc Khoury, I
have developed a new algorithm for the metric embedding problem
for graphs<a href="#ref-mars"></a>. Metric-embedding graph drawings directly match the
graph-theoretical distances with the distances in the embedding to
$R^2$. Although in most cases no exact embeddings are possible, a
minimum of an embedding energy can be computed. The state-of-the-art
exact algorithm takes quadratic memory and cubic time (from
requiring the factorization of a fully dense Laplacian matrix).</p>

<p>We build on Drineas et al.'s work on approximate
SVDs<a href="#ref-drineas"></a>.  Via careful algebraic manipulations, we show
how to approximate the Moore-Penrose pseudo-inverse of a dense
Laplacian matrix, requiring that we sample only a small amount of
rows. The exact algorithm is limited, in practice, to about 40,000
vertices (that would require inverting a fully dense matrix
out-of-core). Our technique improves the state-of-the-art by an order
of magnitude.</p>

<p>The primary insight underlying our algorithm comes from a
linear-algebraic view of the computations, which allows significant
computational advantages. Many techniques in exploratory data
analysis and visualization fail to scale to large data sets because
they hinge on an inner loop over all pairs of data points (in a
sense because they try to capture something fundamental about
the <i>relation</i> between points); I expect this idea of efficient
low-rank approximation to have applicability in visualizastion far
outside of graph drawing. This type of shift in perspective is
illustrative of much of what I like about my research.</p>

</div></div>

<div class="row">
<h1>VERIFICATION: BUILDING TRUST IN THE VISUALIZATION PROCESS</h1>
</div>

<div class="row"><div class="col-md-7">

<p>
When doing exploratory data analysis, analysts rely on a variety of 
algorithms to build a mental model of the problem and data at hand. It
is crucial that they can trust the results they get in the analysis:
bugs in implementations can be a threat to viability of data analysis
of complex datasets in the same way as slow implementations are a
threat to viability of data analysis of massive data. In the context 
of scientific visualization, I have worked to bring the framework of
verification and validation<a href="#ref-vnv"></a> to scientific
visualization. We have designed verification procedures for 
foundational techniques in visualization, like isosurface
extraction<a href="#ref-iso-vnv, ref-topo-vnv"></a> and direct volume rendering<a href="#ref-iso-dvr"></a>.
Remarkably, some of the verification work has been used to show that a
widely-used algorithm and proof for extracting isosurfaces with
prescribed topology had an error in the literature gone unnoticed for
more than a decade, which translated in practice to two independently
published erroneous implementations<a href="#ref-mc33-fix"></a>.
</p>

<p>Just as interestingly, the process of exploratory visualization can
itself be modeled as an algorithm that takes data as an input and
produces something that is observed by the analyst, and from which
they draw some conclusion. How do we verify <i>this</i> process, and
increase (or qualify) our trust in its results?  Gelman and Shalizi
have argued for revision of statistical models to play a central role
in the modern practice of Bayesian statistics, <i>together with
  model checking based on data</i><a href="#ref-gelman-shalizi"></a>. In my view,
interactive visualization should not get a similar pass: I'm
interested in understanding our reliance in exploratory and
interactive visualization, and specifically in using resampling
methods for this evaluation<a href="#ref-bayes-tears"></a>.
</p>

</div></div>

<div class="row">
<h1>COLLABORATIVE DATA ANALYSIS</h1>
</div>

<div class="row"><div class="col-md-7">

<p>Big Data can be Big because it is massive, but it can also be Big
because it is messy. Data sources are dirty,
noisy, have missing values, and change over time. All of these are
barriers to data-driven science, and are absolutely prevalent in real-life
usage. In this regime, I believe that the main barrier to insight is not
our algorithms, but rather how quickly we can explore the data.
</p>

<p>
Tools for enabling collaboration should be central to the practice of
modern data analysis, in no small part because the task of creating
and evaluating models is costly, but can be parallelized.  At
AT&amp;T, I experienced this first hand; to better understand the needs
of data analysts, I embedded myself in teams competing in data mininag
competitions, where we placed among the award-winning teams in two
separate occasions<a href="#ref-false-positives"></a>. I have come to believe
that the success or failure of teams in these competitions lie in the
ability to quickly iterate on new ideas, compare them to previous
work, visualize and understand the relationships, and so on.
</p>

<p>One of my recent open-source projects is RCloud<a href="#ref-rcloud"></a>, an
  environment for collaborative data analysis I am developing in
  collaboration with Simon Urbanek and Gordon Woodhull.
  RCloud provides basic features
  like integrated, transparent version control, lets users of the
  system search all scripts created by any user, and lets users comment on and
  rate their favorite R scripts. One way to see RCloud is as a novel
  synthesis of modern collaborative software-development environments like
  jsfiddle<a href="#ref-jsfiddle"></a>, transparent access to versioning tools
  like VisTrails<a href="#ref-vistrails"></a> and Github
  gists<a href="#ref-github-gists"></a>, and powerful web-based data analysis
  like the iPython web notebook<a href="#ref-ipython-web"></a>.
  RCloud is being used at AT&amp;T, and quickly changed how
  researchers collaborate. Almost overnight, instead of sharing
  screenshots or static figures, users started organically including
  links to RCloud notebooks in internal presentations, emails, and
  similar exchanges. 
</p>

<p>Reducing the friction between creating, exploring and sharing
  visualizations has the potential to significantly change how
  data-driven research and collaborations happen, and intend to continue
  work to enable this vision. Visualization has become popular, but as
  an exploratory activity, it is still underused, misunderstood,
  limited, and isolated. Through my scholarly work, my goal is to
  empower data-driven analysts to better understand and act on the
  world.</p>

<!-- \bibliographystyle{abbrv} -->
<!-- \bibliography{paper} -->

</div></div>
